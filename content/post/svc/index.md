---
title: PAC-Learning for Strategic Classification
subtitle: "Ravi Sundaram, Anil Vullikanti, Haifeng Xu, Fan Yao"
date: 2020-12-13T00:00:00Z
summary: We establish a unified framework for strategic classification problems and introduce the notion of strategic VC-dimension (SVC) to capture its PAC-learnability.
draft: false
featured: false
authors:
  - admin
lastmod: 2020-12-13T00:00:00Z
tags:
  - Academic
categories:
  - poster
  - ""
projects: []
image:
<!--  caption: "Image credit: (https://bkt-fig-65175.oss-us-east-1.aliyuncs.com/research/poster/poster_svc.jpeg)"-->
  focal_point: ""
  placement: 2
  preview_only: false
---





## Poster

![po](https://bkt-fig-65175.oss-us-east-1.aliyuncs.com/research/poster/poster_svc.jpeg)

The study of strategic or adversarial manipulation of testing data to
  fool a classifier has attracted much recent attention. Most previous works
  have focused on two extreme situations where any testing data point either is
  completely adversarial or always equally prefers the positive label. In this
  paper, we generalize both of these through a unified framework for strategic
  classification and introduce the notion of strategic VC-dimension (SVC) to
  capture the PAC-learnability in our general strategic setup. SVC provably
  generalizes the recent concept of adversarial VC-dimension (AVC) introduced by
  Cullina et al. (2018). We instantiate our framework for the fundamental
  strategic linear classification problem. We fully characterize: (1) the
  statistical learnability of linear classifiers by pinning down its SVC; (2)
  it's computational tractability by pinning down the complexity of the
  empirical risk minimization problem. Interestingly, the SVC of linear
  classifiers is always upper bounded by its standard VC-dimension. This
  characterization also strictly generalizes the AVC bound for linear
  classifiers (Cullina et al., 2018).



